<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Parallel Usage: Spawning workers from the command line &#8212; AutoSklearn 0.15.0 documentation</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/bootstrap-sphinx.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-rendered-html.css" />
    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">
<script type="text/javascript" src="../../_static/js/jquery-1.12.4.min.js"></script>
<script type="text/javascript" src="../../_static/js/jquery-fix.js"></script>
<script type="text/javascript" src="../../_static/bootstrap-3.4.1/js/bootstrap.min.js"></script>
<script type="text/javascript" src="../../_static/bootstrap-sphinx.js"></script>

  </head><body>
  
  <a href="https://github.com/automl/auto-sklearn"
     class="visible-desktop hidden-xs"><img
    id="gh-banner"
    style="position: absolute; top: 50px; right: 0; border: 0;"
    src="https://s3.amazonaws.com/github/ribbons/forkme_right_red_aa0000.png"
    alt="Fork me on GitHub"></a>
  <script>
    // Adjust banner height.
    $(function () {
      var navHeight = $(".navbar .container").css("height");
      $("#gh-banner").css("top", navHeight);
    });
  </script>


  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../index.html">
          auto-sklearn</a>
        <span class="navbar-text navbar-version pull-left"><b>0.15.0</b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
                <li><a href="../../index.html">Start</a></li>
                <li><a href="../../releases.html">Releases</a></li>
                <li><a href="../../installation.html">Installation</a></li>
                <li><a href="../../manual.html">Manual</a></li>
                <li><a href="../index.html">Examples</a></li>
                <li><a href="../../api.html">API</a></li>
                <li><a href="../../extending.html">Extending</a></li>
                <li><a href="../../faq.html">FAQ</a></li>
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../../index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"></ul>
</li>
              
            
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
      <div class="col-md-3">
        <div id="sidebar" class="bs-sidenav" role="complementary"><ul>
<li><a class="reference internal" href="#">Parallel Usage: Spawning workers from the command line</a><ul>
<li><a class="reference internal" href="#background">Background</a></li>
<li><a class="reference internal" href="#import-statements">Import statements</a></li>
<li><a class="reference internal" href="#setup-client-scheduler-communication">0. Setup client-scheduler communication</a></li>
<li><a class="reference internal" href="#start-scheduler">1. Start scheduler</a></li>
<li><a class="reference internal" href="#start-two-workers">2. Start two workers</a></li>
<li><a class="reference internal" href="#creating-a-client-in-python">3. Creating a client in Python</a><ul>
<li><a class="reference internal" href="#start-auto-sklearn">Start Auto-sklearn</a></li>
</ul>
</li>
<li><a class="reference internal" href="#wait-until-all-workers-are-closed">Wait until all workers are closed</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    <div class="body col-md-9 content" role="main">
      
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p>Click <a class="reference internal" href="#sphx-glr-download-examples-60-search-example-parallel-manual-spawning-cli-py"><span class="std std-ref">here</span></a>
to download the full example code or to run this example in your browser via Binder</p>
</div>
<div class="sphx-glr-example-title section" id="parallel-usage-spawning-workers-from-the-command-line">
<span id="sphx-glr-examples-60-search-example-parallel-manual-spawning-cli-py"></span><h1>Parallel Usage: Spawning workers from the command line<a class="headerlink" href="#parallel-usage-spawning-workers-from-the-command-line" title="Permalink to this headline">¶</a></h1>
<p><em>Auto-sklearn</em> uses
<a class="reference external" href="https://distributed.dask.org/en/latest/index.html">dask.distributed</a>
for parallel optimization.</p>
<p>This example shows how to start the dask scheduler and spawn
workers for <em>Auto-sklearn</em> manually from the command line. Use this example
as a starting point to parallelize <em>Auto-sklearn</em> across multiple
machines.</p>
<p>To run <em>Auto-sklearn</em> in parallel on a single machine check out the example
<a class="reference internal" href="example_parallel_n_jobs.html#sphx-glr-examples-60-search-example-parallel-n-jobs-py"><span class="std std-ref">Parallel Usage  on a single machine</span></a>.</p>
<p>If you want to start everything manually from within Python
please see <code class="docutils literal notranslate"><span class="pre">:ref:sphx_glr_examples_60_search_example_parallel_manual_spawning_python.py</span></code>.</p>
<p><strong>NOTE:</strong> Above example is disabled due to issue <a class="reference external" href="https://github.com/dask/distributed/issues/5627">https://github.com/dask/distributed/issues/5627</a></p>
<p>You can learn more about the dask command line interface from
<a class="reference external" href="https://docs.dask.org/en/latest/setup/cli.html">https://docs.dask.org/en/latest/setup/cli.html</a>.</p>
<p>When manually passing a dask client to Auto-sklearn, all logic
must be guarded by <code class="docutils literal notranslate"><span class="pre">if</span> <span class="pre">__name__</span> <span class="pre">==</span> <span class="pre">&quot;__main__&quot;:</span></code> statements! We use
multiple such statements to properly render this example as a notebook
and also allow execution via the command line.</p>
<div class="section" id="background">
<h2>Background<a class="headerlink" href="#background" title="Permalink to this headline">¶</a></h2>
<p>To run Auto-sklearn distributed on multiple machines we need to set
up three components:</p>
<ol class="arabic simple">
<li><p><strong>Auto-sklearn and a dask client</strong>. This will manage all workload, find new
configurations to evaluate and submit jobs via a dask client. As this
runs Bayesian optimization it should be executed on its own CPU.</p></li>
<li><p><strong>The dask workers</strong>. They will do the actual work of running machine
learning algorithms and require their own CPU each.</p></li>
<li><p><strong>The scheduler</strong>. It manages the communication between the dask client
and the different dask workers. As the client and all workers connect
to the scheduler it must be started first. This is a light-weight job
and does not require its own CPU.</p></li>
</ol>
<p>We will now start these three components in reverse order: scheduler,
workers and client. Also, in a real setup, the scheduler and the workers should
be started from the command line and not from within a Python file via
the <code class="docutils literal notranslate"><span class="pre">subprocess</span></code> module as done here (for the sake of having a self-contained
example).</p>
</div>
<div class="section" id="import-statements">
<h2>Import statements<a class="headerlink" href="#import-statements" title="Permalink to this headline">¶</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">multiprocessing</span>
<span class="kn">import</span> <span class="nn">subprocess</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="kn">import</span> <span class="nn">dask.distributed</span>
<span class="kn">import</span> <span class="nn">sklearn.datasets</span>
<span class="kn">import</span> <span class="nn">sklearn.metrics</span>

<span class="kn">from</span> <span class="nn">autosklearn.classification</span> <span class="kn">import</span> <span class="n">AutoSklearnClassifier</span>
<span class="kn">from</span> <span class="nn">autosklearn.constants</span> <span class="kn">import</span> <span class="n">MULTICLASS_CLASSIFICATION</span>

<span class="n">tmp_folder</span> <span class="o">=</span> <span class="s2">&quot;/tmp/autosklearn_parallel_3_example_tmp&quot;</span>

<span class="n">worker_processes</span> <span class="o">=</span> <span class="p">[]</span>
</pre></div>
</div>
</div>
<div class="section" id="setup-client-scheduler-communication">
<h2>0. Setup client-scheduler communication<a class="headerlink" href="#setup-client-scheduler-communication" title="Permalink to this headline">¶</a></h2>
<p>In this examples the dask scheduler is started without an explicit
address and port. Instead, the scheduler takes a free port and stores
relevant information in a file for which we provided the name and
location. This filename is also given to the worker so they can find all
relevant information to connect to the scheduler.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scheduler_file_name</span> <span class="o">=</span> <span class="s2">&quot;scheduler-file.json&quot;</span>
</pre></div>
</div>
</div>
<div class="section" id="start-scheduler">
<h2>1. Start scheduler<a class="headerlink" href="#start-scheduler" title="Permalink to this headline">¶</a></h2>
<p>Starting the scheduler is done with the following bash command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>dask-scheduler --scheduler-file scheduler-file.json --idle-timeout <span class="m">10</span>
</pre></div>
</div>
<p>We will now execute this bash command from within Python to have a
self-contained example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">cli_start_scheduler</span><span class="p">(</span><span class="n">scheduler_file_name</span><span class="p">):</span>
    <span class="n">command</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;dask-scheduler --scheduler-file </span><span class="si">{</span><span class="n">scheduler_file_name</span><span class="si">}</span><span class="s2"> --idle-timeout 10&quot;</span>
    <span class="n">proc</span> <span class="o">=</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
        <span class="n">command</span><span class="p">,</span>
        <span class="n">stdout</span><span class="o">=</span><span class="n">subprocess</span><span class="o">.</span><span class="n">PIPE</span><span class="p">,</span>
        <span class="n">stderr</span><span class="o">=</span><span class="n">subprocess</span><span class="o">.</span><span class="n">STDOUT</span><span class="p">,</span>
        <span class="n">shell</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">check</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">while</span> <span class="n">proc</span><span class="o">.</span><span class="n">returncode</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">process_python_worker</span> <span class="o">=</span> <span class="n">multiprocessing</span><span class="o">.</span><span class="n">Process</span><span class="p">(</span>
        <span class="n">target</span><span class="o">=</span><span class="n">cli_start_scheduler</span><span class="p">,</span>
        <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">scheduler_file_name</span><span class="p">,),</span>
    <span class="p">)</span>
    <span class="n">process_python_worker</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
    <span class="n">worker_processes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">process_python_worker</span><span class="p">)</span>

    <span class="c1"># Wait a second for the scheduler to become available</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="start-two-workers">
<h2>2. Start two workers<a class="headerlink" href="#start-two-workers" title="Permalink to this headline">¶</a></h2>
<p>Starting the scheduler is done with the following bash command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">DASK_DISTRIBUTED__WORKER__DAEMON</span><span class="o">=</span>False <span class="se">\</span>
    dask-worker --nthreads <span class="m">1</span> --lifetime <span class="m">35</span> --memory-limit <span class="m">0</span> <span class="se">\</span>
    --scheduler-file scheduler-file.json
</pre></div>
</div>
<p>We will now execute this bash command from within Python to have a
self-contained example. Please note, that
<code class="docutils literal notranslate"><span class="pre">DASK_DISTRIBUTED__WORKER__DAEMON=False</span></code> is required in this
case as dask-worker creates a new process, which by default is not
compatible with Auto-sklearn creating new processes in the workers itself.
We disable dask’s memory management by passing <code class="docutils literal notranslate"><span class="pre">--memory-limit</span></code> as
Auto-sklearn does the memory management itself.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">cli_start_worker</span><span class="p">(</span><span class="n">scheduler_file_name</span><span class="p">):</span>
    <span class="n">command</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">&quot;DASK_DISTRIBUTED__WORKER__DAEMON=False &quot;</span>
        <span class="s2">&quot;dask-worker --nthreads 1 --lifetime 35 --memory-limit 0 &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;--scheduler-file </span><span class="si">{</span><span class="n">scheduler_file_name</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="n">proc</span> <span class="o">=</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
        <span class="n">command</span><span class="p">,</span> <span class="n">stdout</span><span class="o">=</span><span class="n">subprocess</span><span class="o">.</span><span class="n">PIPE</span><span class="p">,</span> <span class="n">stderr</span><span class="o">=</span><span class="n">subprocess</span><span class="o">.</span><span class="n">STDOUT</span><span class="p">,</span> <span class="n">shell</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="k">while</span> <span class="n">proc</span><span class="o">.</span><span class="n">returncode</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
        <span class="n">process_cli_worker</span> <span class="o">=</span> <span class="n">multiprocessing</span><span class="o">.</span><span class="n">Process</span><span class="p">(</span>
            <span class="n">target</span><span class="o">=</span><span class="n">cli_start_worker</span><span class="p">,</span>
            <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">scheduler_file_name</span><span class="p">,),</span>
        <span class="p">)</span>
        <span class="n">process_cli_worker</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
        <span class="n">worker_processes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">process_cli_worker</span><span class="p">)</span>

    <span class="c1"># Wait a second for workers to become available</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="creating-a-client-in-python">
<h2>3. Creating a client in Python<a class="headerlink" href="#creating-a-client-in-python" title="Permalink to this headline">¶</a></h2>
<p>Finally we create a dask cluster which also connects to the scheduler via
the information in the file created by the scheduler.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">client</span> <span class="o">=</span> <span class="n">dask</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">Client</span><span class="p">(</span><span class="n">scheduler_file</span><span class="o">=</span><span class="n">scheduler_file_name</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="start-auto-sklearn">
<h3>Start Auto-sklearn<a class="headerlink" href="#start-auto-sklearn" title="Permalink to this headline">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">load_breast_cancer</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">model_selection</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span>
    <span class="p">)</span>

    <span class="n">automl</span> <span class="o">=</span> <span class="n">AutoSklearnClassifier</span><span class="p">(</span>
        <span class="n">delete_tmp_folder_after_terminate</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">time_left_for_this_task</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
        <span class="n">per_run_time_limit</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">memory_limit</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span>
        <span class="n">tmp_folder</span><span class="o">=</span><span class="n">tmp_folder</span><span class="p">,</span>
        <span class="n">seed</span><span class="o">=</span><span class="mi">777</span><span class="p">,</span>
        <span class="c1"># n_jobs is ignored internally as we pass a dask client.</span>
        <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="c1"># Pass a dask client which connects to the previously constructed cluster.</span>
        <span class="n">dask_client</span><span class="o">=</span><span class="n">client</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">automl</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="n">automl</span><span class="o">.</span><span class="n">fit_ensemble</span><span class="p">(</span>
        <span class="n">y_train</span><span class="p">,</span>
        <span class="n">task</span><span class="o">=</span><span class="n">MULTICLASS_CLASSIFICATION</span><span class="p">,</span>
        <span class="n">dataset_name</span><span class="o">=</span><span class="s2">&quot;digits&quot;</span><span class="p">,</span>
        <span class="n">ensemble_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;ensemble_size&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">},</span>
        <span class="n">ensemble_nbest</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">predictions</span> <span class="o">=</span> <span class="n">automl</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">automl</span><span class="o">.</span><span class="n">sprint_statistics</span><span class="p">())</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy score&quot;</span><span class="p">,</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>auto-sklearn results:
  Dataset name: 2182d124-2217-11ed-87ec-fd3bb3cfc12d
  Metric: accuracy
  Best validation score: 0.992908
  Number of target algorithm runs: 13
  Number of successful target algorithm runs: 13
  Number of crashed target algorithm runs: 0
  Number of target algorithms that exceeded the time limit: 0
  Number of target algorithms that exceeded the memory limit: 0

Accuracy score 0.958041958041958
</pre></div>
</div>
</div>
</div>
<div class="section" id="wait-until-all-workers-are-closed">
<h2>Wait until all workers are closed<a class="headerlink" href="#wait-until-all-workers-are-closed" title="Permalink to this headline">¶</a></h2>
<p>This is only necessary if the workers are started from within this python
script. In a real application one would start them directly from the command
line.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">process_python_worker</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">process</span> <span class="ow">in</span> <span class="n">worker_processes</span><span class="p">:</span>
        <span class="n">process</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
</pre></div>
</div>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> ( 0 minutes  45.649 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-examples-60-search-example-parallel-manual-spawning-cli-py">
<div class="binder-badge docutils container">
<a class="reference external image-reference" href="https://mybinder.org/v2/gh/automl/auto-sklearn/master?urlpath=lab/tree/notebooks/examples/60_search/example_parallel_manual_spawning_cli.ipynb"><img alt="Launch binder" src="../../_images/binder_badge_logo2.svg" width="150px" /></a>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/baf53fc945368668a0cd202acebc6220/example_parallel_manual_spawning_cli.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">example_parallel_manual_spawning_cli.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/c6746d1b897496495baebd219e94d74e/example_parallel_manual_spawning_cli.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">example_parallel_manual_spawning_cli.ipynb</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</div>
</div>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
        <br/>
        
<div id="sourcelink">
  <a href="../../_sources/examples/60_search/example_parallel_manual_spawning_cli.rst.txt"
     rel="nofollow">Source</a>
</div>
      
    </p>
    <p>
        &copy; Copyright 2014-2022, Machine Learning Professorship Freiburg.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.2.0.<br/>
    </p>
  </div>
</footer>
  </body>
</html>
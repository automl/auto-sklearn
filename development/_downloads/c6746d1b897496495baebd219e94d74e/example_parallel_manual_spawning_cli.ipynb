{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Parallel Usage: Spawning workers from the command line\n\n*Auto-sklearn* uses\n`dask.distributed <https://distributed.dask.org/en/latest/index.html>`_\nfor parallel optimization.\n\nThis example shows how to start the dask scheduler and spawn\nworkers for *Auto-sklearn* manually from the command line. Use this example\nas a starting point to parallelize *Auto-sklearn* across multiple\nmachines. If you want to start everything manually from within Python\nplease see `this example <example_parallel_manual_spawning_python.html>`_.\nTo run *Auto-sklearn* in parallel on a single machine check out the example\n`Parallel Usage on a single machine <example_parallel_n_jobs.html>`_.\n\nYou can learn more about the dask command line interface from\nhttps://docs.dask.org/en/latest/setup/cli.html.\n\nWhen manually passing a dask client to Auto-sklearn, all logic\nmust be guarded by ``if __name__ == \"__main__\":`` statements! We use\nmultiple such statements to properly render this example as a notebook\nand also allow execution via the command line.\n\n## Background\n\nTo run Auto-sklearn distributed on multiple machines we need to set\nup three components:\n\n1. **Auto-sklearn and a dask client**. This will manage all workload, find new\n   configurations to evaluate and submit jobs via a dask client. As this\n   runs Bayesian optimization it should be executed on its own CPU.\n2. **The dask workers**. They will do the actual work of running machine\n   learning algorithms and require their own CPU each.\n3. **The scheduler**. It manages the communication between the dask client\n   and the different dask workers. As the client and all workers connect\n   to the scheduler it must be started first. This is a light-weight job\n   and does not require its own CPU.\n\nWe will now start these three components in reverse order: scheduler,\nworkers and client. Also, in a real setup, the scheduler and the workers should\nbe started from the command line and not from within a Python file via\nthe ``subprocess`` module as done here (for the sake of having a self-contained\nexample).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import statements\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import multiprocessing\nimport subprocess\nimport time\n\nimport dask.distributed\nimport sklearn.datasets\nimport sklearn.metrics\n\nfrom autosklearn.classification import AutoSklearnClassifier\nfrom autosklearn.constants import MULTICLASS_CLASSIFICATION\n\ntmp_folder = '/tmp/autosklearn_parallel_3_example_tmp'\noutput_folder = '/tmp/autosklearn_parallel_3_example_out'\n\nworker_processes = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Setup client-scheduler communication\n\nIn this examples the dask scheduler is started without an explicit\naddress and port. Instead, the scheduler takes a free port and stores\nrelevant information in a file for which we provided the name and\nlocation. This filename is also given to the worker so they can find all\nrelevant information to connect to the scheduler.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scheduler_file_name = 'scheduler-file.json'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Start scheduler\n\nStarting the scheduler is done with the following bash command:\n\n.. code:: bash\n\n    dask-scheduler --scheduler-file scheduler-file.json --idle-timeout 10\n\nWe will now execute this bash command from within Python to have a\nself-contained example:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def cli_start_scheduler(scheduler_file_name):\n    call_string = (\n        \"dask-scheduler --scheduler-file %s --idle-timeout 10\"\n    ) % scheduler_file_name\n    proc = subprocess.run(call_string, stdout=subprocess.PIPE,\n                          stderr=subprocess.STDOUT, shell=True, check=True)\n    while proc.returncode is None:\n        time.sleep(1)\n\n\nif __name__ == \"__main__\":\n    process_python_worker = multiprocessing.Process(\n        target=cli_start_scheduler,\n        args=(scheduler_file_name, ),\n    )\n    process_python_worker.start()\n    worker_processes.append(process_python_worker)\n\n    # Wait a second for the scheduler to become available\n    time.sleep(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Start two workers\n\nStarting the scheduler is done with the following bash command:\n\n.. code:: bash\n\n    DASK_DISTRIBUTED__WORKER__DAEMON=False \\\n        dask-worker --nthreads 1 --lifetime 35 --memory-limit 0 \\\n        --scheduler-file scheduler-file.json\n\nWe will now execute this bash command from within Python to have a\nself-contained example. Please note, that\n``DASK_DISTRIBUTED__WORKER__DAEMON=False`` is required in this\ncase as dask-worker creates a new process, which by default is not\ncompatible with Auto-sklearn creating new processes in the workers itself.\nWe disable dask's memory management by passing ``--memory-limit`` as\nAuto-sklearn does the memory management itself.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def cli_start_worker(scheduler_file_name):\n    call_string = (\n        \"DASK_DISTRIBUTED__WORKER__DAEMON=False \"\n        \"dask-worker --nthreads 1 --lifetime 35 --memory-limit 0 \"\n        \"--scheduler-file %s\"\n    ) % scheduler_file_name\n    proc = subprocess.run(call_string, stdout=subprocess.PIPE,\n                          stderr=subprocess.STDOUT, shell=True)\n    while proc.returncode is None:\n        time.sleep(1)\n\nif __name__ == '__main__':\n    for _ in range(2):\n        process_cli_worker = multiprocessing.Process(\n            target=cli_start_worker,\n            args=(scheduler_file_name, ),\n        )\n        process_cli_worker.start()\n        worker_processes.append(process_cli_worker)\n\n    # Wait a second for workers to become available\n    time.sleep(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Creating a client in Python\n\nFinally we create a dask cluster which also connects to the scheduler via\nthe information in the file created by the scheduler.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "client = dask.distributed.Client(scheduler_file=scheduler_file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Start Auto-sklearn\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n    X, y = sklearn.datasets.load_breast_cancer(return_X_y=True)\n    X_train, X_test, y_train, y_test = \\\n        sklearn.model_selection.train_test_split(X, y, random_state=1)\n    automl = AutoSklearnClassifier(\n        time_left_for_this_task=30,\n        per_run_time_limit=10,\n        memory_limit=1024,\n        tmp_folder=tmp_folder,\n        output_folder=output_folder,\n        seed=777,\n        # n_jobs is ignored internally as we pass a dask client.\n        n_jobs=1,\n        # Pass a dask client which connects to the previously constructed cluster.\n        dask_client=client,\n    )\n    automl.fit(X_train, y_train)\n\n    automl.fit_ensemble(\n        y_train,\n        task=MULTICLASS_CLASSIFICATION,\n        dataset_name='digits',\n        ensemble_size=20,\n        ensemble_nbest=50,\n    )\n\n    predictions = automl.predict(X_test)\n    print(automl.sprint_statistics())\n    print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Wait until all workers are closed\n\nThis is only necessary if the workers are started from within this python\nscript. In a real application one would start them directly from the command\nline.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n    process_python_worker.join()\n    for process in worker_processes:\n        process.join()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
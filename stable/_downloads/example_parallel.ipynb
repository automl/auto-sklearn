{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Parallel Usage\n\n\n*Auto-sklearn* uses *SMAC* to automatically optimize the hyperparameters of\nthe training models. A variant of *SMAC*, called *pSMAC* (parallel SMAC),\nprovides a means of running several instances of *auto-sklearn* in a parallel\nmode using several computational resources (detailed information of\n*pSMAC* can be found `here <https://automl.github.io/SMAC3/stable/psmac.html>`_).\nThis example shows the necessary steps to configure *auto-sklearn* in\nparallel mode.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import multiprocessing\nimport shutil\n\nimport sklearn.model_selection\nimport sklearn.datasets\nimport sklearn.metrics\n\nfrom autosklearn.metrics import accuracy\nfrom autosklearn.classification import AutoSklearnClassifier\nfrom autosklearn.constants import *\n\ntmp_folder = '/tmp/autosklearn_parallel_example_tmp'\noutput_folder = '/tmp/autosklearn_parallel_example_out'\n\n\nfor dir in [tmp_folder, output_folder]:\n    try:\n        shutil.rmtree(dir)\n    except OSError as e:\n        pass\n\n\ndef get_spawn_classifier(X_train, y_train):\n    def spawn_classifier(seed, dataset_name):\n        \"\"\"Spawn a subprocess.\n\n        auto-sklearn does not take care of spawning worker processes. This\n        function, which is called several times in the main block is a new\n        process which runs one instance of auto-sklearn.\n        \"\"\"\n\n        # Use the initial configurations from meta-learning only in one out of\n        # the four processes spawned. This prevents auto-sklearn from evaluating\n        # the same configurations in four processes.\n        if seed == 0:\n            initial_configurations_via_metalearning = 25\n            smac_scenario_args = {}\n        else:\n            initial_configurations_via_metalearning = 0\n            smac_scenario_args = {'initial_incumbent': 'RANDOM'}\n\n        # Arguments which are different to other runs of auto-sklearn:\n        # 1. all classifiers write to the same output directory\n        # 2. shared_mode is set to True, this enables sharing of data between\n        # models.\n        # 3. all instances of the AutoSklearnClassifier must have a different seed!\n        automl = AutoSklearnClassifier(\n            time_left_for_this_task=60, # sec., how long should this seed fit process run\n            per_run_time_limit=15, # sec., each model may only take this long before it's killed\n            ml_memory_limit=1024, # MB, memory limit imposed on each call to a ML algorithm\n            shared_mode=True, # tmp folder will be shared between seeds\n            tmp_folder=tmp_folder,\n            output_folder=output_folder,\n            delete_tmp_folder_after_terminate=False,\n            ensemble_size=0, # ensembles will be built when all optimization runs are finished\n            initial_configurations_via_metalearning=initial_configurations_via_metalearning,\n            seed=seed,\n            smac_scenario_args=smac_scenario_args,\n        )\n        automl.fit(X_train, y_train, dataset_name=dataset_name)\n    return spawn_classifier\n\n\ndef main():\n\n    X, y = sklearn.datasets.load_digits(return_X_y=True)\n    X_train, X_test, y_train, y_test = \\\n        sklearn.model_selection.train_test_split(X, y, random_state=1)\n\n    processes = []\n    spawn_classifier = get_spawn_classifier(X_train, y_train)\n    for i in range(4): # set this at roughly half of your cores\n        p = multiprocessing.Process(target=spawn_classifier, args=(i, 'digits'))\n        p.start()\n        processes.append(p)\n    for p in processes:\n        p.join()\n\n    print('Starting to build an ensemble!')\n    automl = AutoSklearnClassifier(\n        time_left_for_this_task=15,\n        per_run_time_limit=15,\n        ml_memory_limit=1024,\n        shared_mode=True,\n        ensemble_size=50,\n        ensemble_nbest=200,\n        tmp_folder=tmp_folder,\n        output_folder=output_folder,\n        initial_configurations_via_metalearning=0,\n        seed=1,\n    )\n\n    # Both the ensemble_size and ensemble_nbest parameters can be changed now if\n    # necessary\n    automl.fit_ensemble(\n        y_train,\n        task=MULTICLASS_CLASSIFICATION,\n        metric=accuracy,\n        precision='32',\n        dataset_name='digits',\n        ensemble_size=20,\n        ensemble_nbest=50,\n    )\n\n    predictions = automl.predict(X_test)\n    print(automl.show_models())\n    print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, predictions))\n\n\nif __name__ == '__main__':\n    main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
#! /usr/bin/env python
from collections import deque
import logging
import random
from multiprocessing import Queue
import os
import pkg_resources
import signal
import shutil
import time

import psutil


import autosklearn.start_automl
import autosklearn.util.check_pid
from autosklearn.util import logging_
from autosklearn.util import options


if __name__ == "__main__":
    logging_.setup()
    parser = options.get_options()
    args = parser.parse_args()

    time_limit = args.time_limit
    start_time = time.time()

    # Go to the execution directory
    if args.exec_dir is not None:
        os.chdir(args.exec_dir)

    # Check the output directories
    output_dir = args.output_dir
    tmp_dataset_dir = args.temporary_output_directory

    pid = os.getpid()
    random_number = random.randint(0, 10000)
    remove_output_dir = False
    remove_tmp_dir = False

    if output_dir is None:
        output_dir = "/tmp/autosklearn_output_%d_%d" % (pid, random_number)
        os.makedirs(output_dir)
        remove_output_dir = not args.keep_output
    else:
        if not os.path.isdir(output_dir):
            raise ValueError("If output_dir is specified, it must exist: %s"
                             % output_dir)
        output_dir = os.path.join(output_dir, str(args.seed))
        os.mkdir(output_dir)

    if tmp_dataset_dir is None:
        tmp_dataset_dir = "/tmp/autosklearn_tmp_%d_%d" % (pid, random_number)
        os.makedirs(tmp_dataset_dir)
        remove_tmp_dir = not args.keep_output
    else:
        if not os.path.isdir(tmp_dataset_dir):
            raise ValueError("If tmp_dataset_dir is specified, it must exist: %s"
                             % tmp_dataset_dir)
        tmp_dataset_dir = os.path.join(tmp_dataset_dir)

    logger = logging_.get_logger("autosklearn_%d" % args.seed,
                                 outputdir=tmp_dataset_dir)
    meta_base_logger = logging.getLogger(
        "pyMetaLearn.metalearning.meta_base")
    meta_base_logger.setLevel(logging.DEBUG)
    metalearn_logger = logging.getLogger(
        "pyMetaLearn.optimizers.metalearn_optimizer.metalearner")
    metalearn_logger.setLevel(logging.DEBUG)


    BUFFER = 35  # time-left - BUFFER = timelimit for SMAC/ensemble_script.py
    BUFFER_BEFORE_SENDING_SIGTERM = 30  # We send SIGTERM to all processes
    DELAY_TO_SIGKILL = 15  # And after a delay we send a sigkill

    # Manipulate $PATH so that SMAC and the runsolver are in it.
    smac = pkg_resources.resource_filename(
        "autosklearn",
        "binaries/smac-v2.08.01-development-1/smac-v2.08.01-development-1/")
    runsolver = pkg_resources.resource_filename(
        "autosklearn",
        "binaries/"
    )
    os.environ["PATH"] = smac + os.pathsep + runsolver + os.pathsep + \
                         os.environ["PATH"]

    time_spent_so_far = time.time() - start_time
    time_left_for_smac = time_limit - time_spent_so_far
    queue = Queue()
    automl = autosklearn.start_automl.AutoML(
        queue, args.dataset_name, args.data_dir, tmp_dataset_dir,
        output_dir, time_left_for_smac, args.per_run_time_limit,
        log_dir=tmp_dataset_dir, ensemble_size=args.ensemble_size,
        ensemble_nbest=args.ensemble_nbest,
        initial_configurations_via_metalearning=args
        .metalearning_configurations, seed=args.seed,
        ml_memory_limit=args.ml_memory_limit,
        metadata_directory=args.metadata_directory)
    automl.start_automl()

    [time_needed_to_load_data, data_manager_file, proc_smac, proc_ensembles] \
        = queue.get()

    # == And now we wait till we run out of time
    while time.time() - start_time <= time_limit - BUFFER_BEFORE_SENDING_SIGTERM:
        time.sleep(1)

    # Kill all children, grand-children and so on
    process = psutil.Process()
    # All children which must be killed
    children = deque()
    children.extendleft(process.children(recursive=True))

    for delay, sig in \
            [(0, signal.SIGINT),
             (BUFFER_BEFORE_SENDING_SIGTERM / 3., signal.SIGTERM),
             (BUFFER_BEFORE_SENDING_SIGTERM / 3. * 2., signal.SIGKILL)]:
        visited = set()

        # first, send SIGINT
        while len(children) > 0:
            child = children.pop()
            # First, check if all children of child are in the children set
            if not child.is_running():
                continue

            try:
                grandchildren = process.children(recursive=True)
                for grandchild in grandchildren:
                    if grandchild in visited:
                        continue
                    else:
                        children.appendleft(grandchild)
            except psutil.NoSuchProcess:
                pass

            # Then, send the signal
            try:
                child.send_signal(sig)
            except psutil.NoSuchProcess:
                pass

            visited.add(child)

        # Readd all children we ever found to the list which were running in
        # the last iteration of killing processes to make sure that we killed
        #  them all.
        children.extendleft(visited)

        while time.time() - start_time <= time_limit - \
                (BUFFER_BEFORE_SENDING_SIGTERM - delay):
            time.sleep(1)

    if remove_output_dir is True:
        shutil.rmtree(output_dir)
    if remove_tmp_dir is True:
        shutil.rmtree(tmp_dataset_dir)

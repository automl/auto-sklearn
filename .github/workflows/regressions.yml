name: Regression Tests

on:

  workflow_dispatch:
    inputs:
      config:
        description: 'Config to run'
        required: true
        default: 'small.yml'

  pull_request:
    types: [labeled]

env:
  AUTOMLBENCHMARK_REPO: 'openml/automlbenchmark'
  AUTOMLBENCHMARK_REF: 'master'
  PYTHON_COMPARE_BRANCH: 'master'
  PYTHON_COMPARE_URL: '.github/workflows/dist.yml'
  BASELINE_BRANCH: 'master'
  BASELINE_URL: '.github/workflows/dist.yml'
  GITHUB_URL: 'https://github.com'
  GITHUB_RAW_URL: 'https://raw.githubusercontent.com'

jobs:

  run-regression-tests-labeled:
    name: Run the regression tests
    runs-on: ubuntu-latest
    if: >
      github.event_name == 'workflow_dispatch'
      || (
        github.event_name == 'pull_request'
        && github.event.action == 'labeled'
        && github.event.label.name == 'regression-tests'
      )

    steps:
      - name: Debug
        env:
          GITHUB_CONTEXT: ${{ toJson(github) }}
        run: |
          echo "$GITHUB_CONTEXT"
          pwd
          ls -a

      - name: Create comment
        id: create_comment
        if: github.event_name == 'pull_request'
        uses: peter-evans/create-or-update-comment@v1
        with:
          issue-number: ${{ github.event.pull_request.number }}
          body: |
            Hello @${{ github.event.pull_request.user.login }},
            
            We are doing regression tests for

            * **Branch** ${{ github.event.pull_request.head.ref }}
            * **Commit** ${{ github.event.pull_request.head.sha }}

            [Progress and Artifacts](${{ env.GITHUB_URL }}/${{ github.repository }}/actions/runs/${{ github.run_id }})

            A summary of the results will be show in this comment once complete but the full results will be available once complete.

      - name: Setup Python 3.9.6
        uses: actions/setup-python@v2
        with:
          python-version: 3.9

      - name: Checkout Automlbenchmark
        uses: actions/checkout@v2
        with:
          repository: ${{ env.AUTOMLBENCHMARK_REPO }}
          ref: ${{ env.AUTOMLBENCHMARK_REF }}

      - name: Install Automlbenchmark
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Fetch the Python Script and Baseline from the Repo
        id: fetch
        env:
          pyscript_url: ${{ env.GITHUB_RAW_URL }}/${{github.repository}}/${{ env.PYTHON_COMPARE_BRANCH}}/${{ env.PYTHON_COMPARE_URL }}
          baseline_url: ${{ env.GITHUB_RAW_URL }}/${{github.repository}}/${{ env.BASELINE_BRANCH }}/${{ env.BASELINE_URL }}
          baseline_to: 'baseline.csv'
          pyscript_to: 'compare.py'
        run: |
          wget $pyscript_url -O $pyscript_to
          wget $baseline_url -O $baseline_url

          echo "::set-output name=pyscript::$(pwd)/${pyscript_to}"
          echo "::set-output name=baseline::$(pwd)/${baseline_to}"
          # outpts:
          #   pyscript: path to the pyscript
          #   baseline: path to the baseline

      - name: Start benchmark
        id: benchmark
        env:
          COMMIT_REF: ${{ github.event.pull_request.head.sha }}
          BENCHMARK_ARGS: >-
            autosklearn test -t iris
        run: |
          python runbenchmark.py $BENCHMARK_ARGS
          echo "::set-output name=results_path::$(pwd)/results/results.csv"
          # outputs:
          #   results_path: path to the results of the benchmark

      - name: Run comparison
        id: compare
        env:
          pyscript_path: ${{ steps.fetch.outputs.pyscript }}
          baseline_path: ${{ steps.fetch.outputs.baseline }}
          results_path: ${{ steps.regression_tests.outputs.results_path}}
          filename: comparisons.csv
        run: |
          # python $pyscript_path --baseline $baseline_path --results $results_path --to $filename
          echo "success" > $filename
          cat $pyscript_path >> $filename
          cat $baseline_path >> $filename
          cat $results_path >> $filename
          echo "::set-output name=comparisons_path::$(pwd)/$filename"
          # outputs:
          #   comparisons_path: path to the results of regression test vs baseline

      - name: Upload Results as Artifact
        uses: actions/upload-artifact@v2
        with:
          name: results
          path: |
            ${{ steps.compare.outputs.comparisons_path }}
          retention-days: 1

      - name: Create Comment Body
        id: results_for_comment
        if: >
          github.event_name == 'pull_request'
          && steps.create_comment.outputs.comment-id != ''
        env:
          results_path: ${{ steps.compare.outputs.comparisons_path }}
        run: |
          body="\n**Results**\n$(cat $results_path)
          body="${body//'%'/'%25'}"
          body="${body//$'\n'/'%0A'}"
          body="${body//$'\r'/'%0D'}" 
          echo ::set-output name=body::$body
          # outputs:
          #   body: the body of the comment update

      - name: Update Comment
        id: update_comment
        if: >
          github.event_name == 'pull_request'
          && steps.create_comment.outputs.comment-id != ''
        uses: peter-evans/create-or-update-comment@v1
        with:
          comment-id: ${{ steps.create_comment.outputs.comment-id }}
          body: ${{ steps.results_for_comment.outputs.body }}

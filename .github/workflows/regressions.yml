name: Regression Tests

on:

  workflow_dispatch:
    inputs:
      dummy:
        description: 'Place holder'
        required: false
        default: 'Dummy argument'

  pull_request:
    types: [labeled]

env:
  AUTOMLBENCHMARK_REPO: 'openml/automlbenchmark'
  AUTOMLBENCHMARK_REF: 'master'
  GITHUB_URL: 'https://github.com'
  GITHUB_RAW_URL: 'https://raw.githubusercontent.com'
  WORKFLOW_DIR: '.github/workflows/regression-testing'
  PYTHON_UTIL_FILE: 'regressions-util.py'

jobs:

  create-comment:
  - name: Create comment body
    id: create_comment
    if: >
      github.event_name == 'pull_request' && github.event.action == 'labeled'
    uses: peter-evans/create-or-update-comment@v1
    with:
      issue-number: ${{ github.event.pull_request.number }}
      body: |
        Hello @${{ github.event.pull_request.user.login }},

        We are doing regression tests for

        * **Branch** ${{ github.event.pull_request.head.ref }}
        * **Commit** ${{ github.event.pull_request.head.sha }}

        [Progress and Artifacts](${{ env.GITHUB_URL }}/${{ github.repository }}/actions/runs/${{ github.run_id }})

        A summary of the results will be show in this comment once complete but the full results will be available as an artifact at the above link.

  regression-tests:
  - name: Regression Tests
    runs: on ubuntu-latest
    if: >
      github.event_name == 'workflow_dispatch'
      || (
        github.event_name == 'pull_request'
        && github.event.action == 'labeled'
        && github.event.label.name == 'regression-tests'
      )

    strategy:
      matrix:
        task_type: [regression, classification]

    steps:
      - name: Debug
        env:
          GITHUB_CONTEXT: ${{ toJson(github) }}
        run: |
          echo "$GITHUB_CONTEXT"
          pwd
          ls -a

      - name: Branch extract
        id: extract
        run: echo "##[set-output name=branch;]${GITHUB_REF##*/}"
        # outputs
        #   branch: the branch name

      - name: Download baselines artificats
        uses: dawidd6/action-download-artifact@v2
        with:
          workflow: generate-baselines.yml
          workflow_conclusion: "success"
          branch: master
          name: baselines
        # Downloads
        #   - baseline_classification_x_x_x.csv
        #   - baseline_regression_x_x_x.csv

      - name: Create variables for baselinefiles
        id: baseline_files
        run: |
          BASELINE_CLASSIFICATION=$(ls | grep baseline_classification)
          BASELINE_REGRESSION=$(ls | grep baseline_regression)
          echo "::set-output name=classification::$(pwd)/${BASELINE_CLASSIFICATION}"
          echo "::set-output name=regression::$(pwd)/${BASELINE_REGRESSION}"
        # Outputs:
        #   - classification: path to classification baseline
        #   - regression: path to regression baseline

      - name: Setup Python 3.9.6
        uses: actions/setup-python@v2
        with:
          python-version: 3.9

      - name: Checkout Automlbenchmark
        uses: actions/checkout@v2
        with:
          repository: ${{ env.AUTOMLBENCHMARK_REPO }}
          ref: ${{ env.AUTOMLBENCHMARK_REF }}

      - name: Install Automlbenchmark
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Fetch files
        id: fetch
        env:
          source: ${{ env.GITHUB_RAW_URL }}/${{ github.repository }}/${{ steps.extract.outputs.branch }}/${{ env.WORKFLOW_DIR }}
          benchmark_dir: 'baseline_benchmark'
          util_file: 'regression-util.py'
          config_file: 'config.yaml'
          constraint_file: 'constraints.py'
          benchmark_file: ${{ matrix.task_type }}.yaml
        run: |
          # Get the util file
          wget ${source}/${util_file} -O ${util_file}

          # set up the benchmark directories
          mkdir $benchmark_dir
          mkdir $benchmark_dir/benchmarks
          wget ${source}/${config_file} -O ${benchmark_dir}/${config_file}
          wget ${source}/${constraints_file} -O ${benchmark_dir}/${constraints_file}
          wget ${source}/benchmarks/${benchmark_file} -O ${benchmark_dir}/benchmarks/${benchmark_file}
          echo "::set-output name=benchmarkdir::$(pwd)/${benchmark_dir}"
          echo "::set-output name=util::$(pwd)/${util_file}"
          # outputs
          #   benchmarkdir: the dir with config in it
          #   util: the util filepath

      - name: Create the framework definition for the target branch
        env:
          pyutil: ${{ steps.fetch.outputs.util }}
        run: |-
          # Creates a framework file that automl benchmark can use
          python $pyutil --generate-framework-def \
            --userdir ${{ steps.fetch.outputs.benchmarkdir }}
            --owner ${{ github.repository_owner }} \
            --branch ${{ steps.extract.outputs.branch }} \
            --commit ${{ github.sha }}

      - name: Start benchmark
        id: benchmark
        env:
          benchmarkdir: ${{ steps.fetch.outputs.benchmarkdir }}
          framework: autosklearn_targeted
          benchmark: ${{ matrix.task_type }}
          constraint: 10fold10min
          benchmark_output: results/results.csv
          results_to: baseline_${{ matrix.task_type }}_${{ github.repository_owner }}_${{ steps.extract.outputs.branch }}_${{ github.sha }}.csv
        run: |
          python runbenchmark.py -u $benchmarkdir $framework $benchmark $constraint
          ls
          ls results
          cat results/results.csv
          mv $benchmark_output $results_to
          echo "::set-output name=results_path::$(pwd)/${results_to}"
          # outputs:
          #   results_path: path to the results of the benchmark

      - name: Upload ${{matrix.task_type}} results as artifact
        uses: actions/upload-artifact@v2
        with:
          name: results_${{ matrix.task_type }}
          path: |
            ${{ steps.benchmark.outputs.results_path }}
          retention-days: 90

